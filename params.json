{"name":"Xeon-CafPhi","tagline":"Caffe deep learning framework - optimized for Xeon Phi","body":"# Project Proposal\r\n\r\n## Summary\r\nOptimize Caffe deep learning framework on [Xeon Phi](http://ark.intel.com/products/71992/Intel-Xeon-Phi-Coprocessor-5110P-8GB-1_053-GHz-60-core). Optimization will be done by evaluating the performance of CIFAR-10 and MNIST data sets. We also plan to make a detailed analysis on the performance of Caffe on Xeon Phi (with and without GPU support).\r\n\r\n## Background\r\n\r\n![](http://deeplearning.net/tutorial/_images/mylenet.png)\r\n\r\nA convolutional neural network is currently a very famous Machine Learning algorithm which is used to classify data. It is especially used in computer vision to classify images with high accuracy. A convolutional neural network applied on an image has multiple axes of parallelism at the different layers of the propagation during the training and testing of the network.\r\n\r\nEach hidden layer has multiple feature maps, which are produced by convolving respective weights weighed over all the pixels of the previous layerâ€™s feature maps and adding corresponding results. This convolution task is highly parallelisable. \r\n\r\nMaxPooling is a task of subsampling the image by taking the average or maximum amongst subparts of the images. This is done in order to avoid any variance. This task of down-sampling images is also highly parallelisable.\r\n\r\nCaffe is a framework written in C++ to implement this convolutional neural network, in order to utilise these axes of parallelism available. Caffe takes the convolutions, uses cblas library to turn them into a bunch of matrix multiplications and the uses CudaDNN with Nvidia GPUs to attain faster training speeds.\r\n\r\n\r\n## Challenge\r\nOptimizing Caffe for Xeon Phi is hard as it is already optimized for both generic CPU and GPU. We intend to find specific parts of Caffe implementation that can be optimized to better use the features of Xeon Phi which will be challenging to find. This will require us to understand the existing parallelism support and find new parallelism or fine tune existing support to that of Xeon Phi. \r\n\r\nMy partner and I have not used or worked with Caffe before and hence we will need to spend time to use and understand the code. Convolution implementation is done by transforming the input data to matrices and computation is done on these. We would need to understand and reimplement this feature to better utilize parallelism of Xeon Phi, which will be challenging.\r\n\r\n## Resources\r\nHardware: We will be using Latedays cluster machine that has Xeon Phi co-processor board. We will also be using our laptops to perform initial testing. \r\nSoftware: Caffe software which comes with BSD 2-Clause license will be our starting point for this project. Test Data sets: CIFAR-10 and MNIST will also be required for evaluation.\r\n\r\n## Goals/Deliverables:\r\nPrimary goal is to understand and modify the Caffe codebase to best utilize all the resources available in Xeon Phi. We will be evaluating performance with CIFAR-10 and MNIST data sets and comparing the same with default Caffe code run on the same machine.\r\n\r\nOverall, we would like to gain information about performance benefits of optimizing Caffe on a powerful board such as Xeon Phi. We would also like to gain information about the performance drawbacks of running these optimized code on other machines such as our laptops.\r\n\r\nA possible extension is to explore the optimization of Caffe code with GPU support on Latedays cluster.\r\n\r\n## Platforms\r\nWe will be using the Linux posix threads framework and ISPC framework to parallelise the caffe implementation for the Xeon Phi processor of Latedays cluster. We will also consider OpenMPI to leverage other 14 cores on Latedays cluster.\r\n\r\n\r\n## Schedule\r\n* Friday, April 10: We shall port the caffe framework on Latedays Xeon Phi cluster and apply it to CIFAR10 dataset.\r\n* Friday, April 17: Next we wish to switch on the GPU capability of Caffe and find means to measure its performance.\r\n* We will time the implementation, recognise the the bottlenecks for both serial and GPU implementation of Caffe on latedays, also recognise opportunities of parallelism utilised by GPU matrix multiplication method.\r\n* Friday, April 24: Decide on the implementation on XeonPhi via matrix multiplication or convolution. We will implement the Caffe CNN using ISPC on the XeonPhi. Optimise the implementation keeping in mind both the cache coherence and the bandwidth.\r\n* Friday, May 1: We time our implementation, check for correctness and race the caffe CPU vs GPU implementation.\r\n* Thursday, May 7: Buffer week. Add any additional features if time permits (Combining GPU and CPU features on Latedays cluster).\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}